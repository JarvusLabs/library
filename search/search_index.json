{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u00b6 The Jarvus Labs\u2019 Library serves as a hub for Jarvus\u2019 open know-how.","title":"Welcome"},{"location":"#welcome","text":"The Jarvus Labs\u2019 Library serves as a hub for Jarvus\u2019 open know-how.","title":"Welcome"},{"location":"kubernetes/recipes/restic-snapshots/","text":"Restic Snapshots \u00b6 Install restic binary \u00b6 Install the latest restic binary from your package manage of choice or via Jarvus\u2019 Chef Habitat package: hab pkg install --binlink jarvus/restic Provision bucket \u00b6 Create a bucket and obtain access credentials at a low-cost cloud object storage host: Linode Add an Object Storage bucket Create an Access key Backblaze Provision B2 bucket Select Private Enable Object Lock Provision B2 app key Use same name as bucket Only allow access to created bucket Select Read and Write access Do not allow listing all bucket names Do not set file name prefix or duration Build restic environment \u00b6 Create a secure file to store needed environment variables for the restic client to read and write to the encrypted repository bucket: Linode RESTIC_REPOSITORY = s3:us-east-1.linodeobjects.com/restic-myhost RESTIC_PASSWORD = # Access Key: AWS_ACCESS_KEY_ID = # Secret Key: AWS_SECRET_ACCESS_KEY = Backblaze RESTIC_REPOSITORY = b2:restic-myhost RESTIC_PASSWORD = # keyID: B2_ACCOUNT_ID = # applicationKey: B2_ACCOUNT_KEY = Create ~/.restic/myhost.env from above template Tailor RESTIC_REPOSITORY to created bucket Generate RESTIC_PASSWORD and save to credential vault Fill storage credentials Secure configuration: sudo chmod 600 ~/.restic/myhost.env Initialize repository \u00b6 Load the environment into your current shell to run Restic\u2019s one-time init command to verify connection details and set up the encrypted repository structure within the bucket: set -a ; ~/.restic/myhost.env ; set +a restic init Create Kubernetes Secret \u00b6 Connect to the target cluster and create a new secret from the above .env file: kubectl -n myhost \\ create secret generic restic \\ --from-env-file = $HOME /.restic/myhost.env Create Kubernetes CronJob \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : myapp-cron namespace : myapp --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : myapp-cron namespace : myapp rules : - apiGroups : - '' resources : - pods verbs : - get - watch - list - apiGroups : - '' resources : - pods/exec verbs : - create --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : myapp-cron namespace : myapp roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : myapp-cron subjects : - kind : ServiceAccount name : myapp-cron --- apiVersion : batch/v1beta1 kind : CronJob metadata : name : myapp-hourly namespace : myapp spec : concurrencyPolicy : Forbid schedule : 40 * * * * startingDeadlineSeconds : 86400 jobTemplate : spec : activeDeadlineSeconds : 7200 template : spec : serviceAccountName : myapp-cron restartPolicy : Never containers : - name : kubectl image : lachlanevenson/k8s-kubectl:v1.18.16 imagePullPolicy : IfNotPresent envFrom : - secretRef : name : restic args : command : - /bin/sh - '-c' - > # resolve pod name for running instance (there should just be one) pod_name=$(kubectl get pod \\ -l app.kubernetes.io/instance=myapp \\ --field-selector=status.phase==Running \\ -o jsonpath='{.items[0].metadata.name}' ) # snapshot mysql database and site data to remote restic repository kubectl exec \"${pod_name}\" -- bash -c \" # configure restic repository export B2_ACCOUNT_ID='${B2_ACCOUNT_ID}' export B2_ACCOUNT_KEY='${B2_ACCOUNT_KEY}' export RESTIC_REPOSITORY='${RESTIC_REPOSITORY}' export RESTIC_PASSWORD='${RESTIC_PASSWORD}' # install CLI dependencies hab pkg install jarvus/restic core/gzip core/curl # get current database name database_name=\\\"\\$(hab pkg exec myorigin/myapp-composite mysql -srNe 'SELECT SCHEMA()')\\\" # snapshot current database echo \\\"Snapshotting database: \\${database_name}\\\" hab pkg exec myorigin/myapp-composite \\ mysqldump \\ --default-character-set=utf8 \\ --force \\ --single-transaction \\ --quick \\ --compact \\ --extended-insert \\ --order-by-primary \\ --ignore-table=\\\"\\${database_name}.sessions\\\" \\ \\\"\\${database_name}\\\" \\ | hab pkg exec core/gzip gzip --rsyncable \\ | hab pkg exec jarvus/restic restic backup \\ --host 'myapp' \\ --stdin \\ --stdin-filename database.sql.gz # snapshot data echo 'Snapshoting site data' hab pkg exec jarvus/restic restic backup \\ /hab/svc/myapp/data \\ --host 'myapp' \\ --exclude='*.log' \\ --exclude='/hab/svc/myapp/data/media/*x*/**' # prune aged snapshots echo 'Pruning snapshots' hab pkg exec jarvus/restic restic forget \\ --host 'myapp' \\ --keep-last 3 \\ --keep-daily 7 \\ --keep-weekly 52 \"","title":"Restic Snapshots"},{"location":"kubernetes/recipes/restic-snapshots/#restic-snapshots","text":"","title":"Restic Snapshots"},{"location":"kubernetes/recipes/restic-snapshots/#install-restic-binary","text":"Install the latest restic binary from your package manage of choice or via Jarvus\u2019 Chef Habitat package: hab pkg install --binlink jarvus/restic","title":"Install restic binary"},{"location":"kubernetes/recipes/restic-snapshots/#provision-bucket","text":"Create a bucket and obtain access credentials at a low-cost cloud object storage host: Linode Add an Object Storage bucket Create an Access key Backblaze Provision B2 bucket Select Private Enable Object Lock Provision B2 app key Use same name as bucket Only allow access to created bucket Select Read and Write access Do not allow listing all bucket names Do not set file name prefix or duration","title":"Provision bucket"},{"location":"kubernetes/recipes/restic-snapshots/#build-restic-environment","text":"Create a secure file to store needed environment variables for the restic client to read and write to the encrypted repository bucket: Linode RESTIC_REPOSITORY = s3:us-east-1.linodeobjects.com/restic-myhost RESTIC_PASSWORD = # Access Key: AWS_ACCESS_KEY_ID = # Secret Key: AWS_SECRET_ACCESS_KEY = Backblaze RESTIC_REPOSITORY = b2:restic-myhost RESTIC_PASSWORD = # keyID: B2_ACCOUNT_ID = # applicationKey: B2_ACCOUNT_KEY = Create ~/.restic/myhost.env from above template Tailor RESTIC_REPOSITORY to created bucket Generate RESTIC_PASSWORD and save to credential vault Fill storage credentials Secure configuration: sudo chmod 600 ~/.restic/myhost.env","title":"Build restic environment"},{"location":"kubernetes/recipes/restic-snapshots/#initialize-repository","text":"Load the environment into your current shell to run Restic\u2019s one-time init command to verify connection details and set up the encrypted repository structure within the bucket: set -a ; ~/.restic/myhost.env ; set +a restic init","title":"Initialize repository"},{"location":"kubernetes/recipes/restic-snapshots/#create-kubernetes-secret","text":"Connect to the target cluster and create a new secret from the above .env file: kubectl -n myhost \\ create secret generic restic \\ --from-env-file = $HOME /.restic/myhost.env","title":"Create Kubernetes Secret"},{"location":"kubernetes/recipes/restic-snapshots/#create-kubernetes-cronjob","text":"apiVersion : v1 kind : ServiceAccount metadata : name : myapp-cron namespace : myapp --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : myapp-cron namespace : myapp rules : - apiGroups : - '' resources : - pods verbs : - get - watch - list - apiGroups : - '' resources : - pods/exec verbs : - create --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : myapp-cron namespace : myapp roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : myapp-cron subjects : - kind : ServiceAccount name : myapp-cron --- apiVersion : batch/v1beta1 kind : CronJob metadata : name : myapp-hourly namespace : myapp spec : concurrencyPolicy : Forbid schedule : 40 * * * * startingDeadlineSeconds : 86400 jobTemplate : spec : activeDeadlineSeconds : 7200 template : spec : serviceAccountName : myapp-cron restartPolicy : Never containers : - name : kubectl image : lachlanevenson/k8s-kubectl:v1.18.16 imagePullPolicy : IfNotPresent envFrom : - secretRef : name : restic args : command : - /bin/sh - '-c' - > # resolve pod name for running instance (there should just be one) pod_name=$(kubectl get pod \\ -l app.kubernetes.io/instance=myapp \\ --field-selector=status.phase==Running \\ -o jsonpath='{.items[0].metadata.name}' ) # snapshot mysql database and site data to remote restic repository kubectl exec \"${pod_name}\" -- bash -c \" # configure restic repository export B2_ACCOUNT_ID='${B2_ACCOUNT_ID}' export B2_ACCOUNT_KEY='${B2_ACCOUNT_KEY}' export RESTIC_REPOSITORY='${RESTIC_REPOSITORY}' export RESTIC_PASSWORD='${RESTIC_PASSWORD}' # install CLI dependencies hab pkg install jarvus/restic core/gzip core/curl # get current database name database_name=\\\"\\$(hab pkg exec myorigin/myapp-composite mysql -srNe 'SELECT SCHEMA()')\\\" # snapshot current database echo \\\"Snapshotting database: \\${database_name}\\\" hab pkg exec myorigin/myapp-composite \\ mysqldump \\ --default-character-set=utf8 \\ --force \\ --single-transaction \\ --quick \\ --compact \\ --extended-insert \\ --order-by-primary \\ --ignore-table=\\\"\\${database_name}.sessions\\\" \\ \\\"\\${database_name}\\\" \\ | hab pkg exec core/gzip gzip --rsyncable \\ | hab pkg exec jarvus/restic restic backup \\ --host 'myapp' \\ --stdin \\ --stdin-filename database.sql.gz # snapshot data echo 'Snapshoting site data' hab pkg exec jarvus/restic restic backup \\ /hab/svc/myapp/data \\ --host 'myapp' \\ --exclude='*.log' \\ --exclude='/hab/svc/myapp/data/media/*x*/**' # prune aged snapshots echo 'Pruning snapshots' hab pkg exec jarvus/restic restic forget \\ --host 'myapp' \\ --keep-last 3 \\ --keep-daily 7 \\ --keep-weekly 52 \"","title":"Create Kubernetes CronJob"},{"location":"kubernetes/shared-clusters/admin-access/","text":"Grant admin access \u00b6 Limited access to a specific namespaced application can be granted for developers and other project contributors to use via a Service Account . Prerequisites \u00b6 An operational shared cluster Uses \u00b6 A service account and associated role can be used, for example, to: Grant read-only access to list pods within a namespace Grant access to execute commands on running pods Grant access to forward network ports to access internal services securely Manifest template \u00b6 In the following template, replace MY_NAMESPACE and MY_ADMIN with the namespace you\u2019re granting access to and a chosen name for the admin account. This template grants any user of the declared service account the three capabilities described above: apiVersion : v1 kind : Namespace metadata : name : MY_NAMESPACE --- apiVersion : v1 kind : ServiceAccount metadata : name : MY_ADMIN namespace : MY_NAMESPACE --- kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : MY_ADMIN namespace : MY_NAMESPACE rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/portforward\" ] verbs : [ \"create\" ] --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : MY_ADMIN namespace : MY_NAMESPACE subjects : - kind : ServiceAccount name : MY_ADMIN namespace : MY_NAMESPACE roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : MY_ADMIN Generate KUBECONFIG \u00b6 After applying the manifest to your cluster, replacing MY_NAMESPACE and MY_ADMIN again with the same values: Install mkkubeconfig command (if needed): sudo hab pkg install --binlink jarvus/mkkubeconfig Use mkkubeconfig to generate and save a KUBECONFIG to disk: mkkubeconfig MY_NAMESPACE MY_ADMIN > ~/.kube/MY_ADMIN.yaml Usage \u00b6 Share the generated KUBECONFIG via a team credential vault or other means Activate the downloaded KUBECONFIG in your current terminal session: export KUBECONFIG = ~/.kube/MY_ADMIN.yaml Get the name of interesting pods, selecting by appropriate label(s), and store them in shell variables: APP_POD = $( kubectl get pod -l component = app -o jsonpath = '{.items[0].metadata.name}' ) DB_POD = $( kubectl get pod -l component = database -o jsonpath = '{.items[0].metadata.name}' ) Open interactive app shell \u00b6 kubectl exec -it $APP_POD -- bash Open interactive database shell \u00b6 kubectl exec -it $DB_POD -- psql -U admin laravel Run an artisan command \u00b6 kubectl exec -it $APP_POD -- php artisan migrate Run an SQL query \u00b6 echo 'SELECT * FROM users' | kubectl exec -i $DB_POD -- psql -U admin laravel Forward PostgreSQL port \u00b6 kubectl port-forward pods/ $DB_POD 5432 :5432 Database logins Default database credentials can usually be found in helm-chart/values.yaml","title":"Grant admin access"},{"location":"kubernetes/shared-clusters/admin-access/#grant-admin-access","text":"Limited access to a specific namespaced application can be granted for developers and other project contributors to use via a Service Account .","title":"Grant admin access"},{"location":"kubernetes/shared-clusters/admin-access/#prerequisites","text":"An operational shared cluster","title":"Prerequisites"},{"location":"kubernetes/shared-clusters/admin-access/#uses","text":"A service account and associated role can be used, for example, to: Grant read-only access to list pods within a namespace Grant access to execute commands on running pods Grant access to forward network ports to access internal services securely","title":"Uses"},{"location":"kubernetes/shared-clusters/admin-access/#manifest-template","text":"In the following template, replace MY_NAMESPACE and MY_ADMIN with the namespace you\u2019re granting access to and a chosen name for the admin account. This template grants any user of the declared service account the three capabilities described above: apiVersion : v1 kind : Namespace metadata : name : MY_NAMESPACE --- apiVersion : v1 kind : ServiceAccount metadata : name : MY_ADMIN namespace : MY_NAMESPACE --- kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : MY_ADMIN namespace : MY_NAMESPACE rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/portforward\" ] verbs : [ \"create\" ] --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : MY_ADMIN namespace : MY_NAMESPACE subjects : - kind : ServiceAccount name : MY_ADMIN namespace : MY_NAMESPACE roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : MY_ADMIN","title":"Manifest template"},{"location":"kubernetes/shared-clusters/admin-access/#generate-kubeconfig","text":"After applying the manifest to your cluster, replacing MY_NAMESPACE and MY_ADMIN again with the same values: Install mkkubeconfig command (if needed): sudo hab pkg install --binlink jarvus/mkkubeconfig Use mkkubeconfig to generate and save a KUBECONFIG to disk: mkkubeconfig MY_NAMESPACE MY_ADMIN > ~/.kube/MY_ADMIN.yaml","title":"Generate KUBECONFIG"},{"location":"kubernetes/shared-clusters/admin-access/#usage","text":"Share the generated KUBECONFIG via a team credential vault or other means Activate the downloaded KUBECONFIG in your current terminal session: export KUBECONFIG = ~/.kube/MY_ADMIN.yaml Get the name of interesting pods, selecting by appropriate label(s), and store them in shell variables: APP_POD = $( kubectl get pod -l component = app -o jsonpath = '{.items[0].metadata.name}' ) DB_POD = $( kubectl get pod -l component = database -o jsonpath = '{.items[0].metadata.name}' )","title":"Usage"},{"location":"kubernetes/shared-clusters/admin-access/#open-interactive-app-shell","text":"kubectl exec -it $APP_POD -- bash","title":"Open interactive app shell"},{"location":"kubernetes/shared-clusters/admin-access/#open-interactive-database-shell","text":"kubectl exec -it $DB_POD -- psql -U admin laravel","title":"Open interactive database shell"},{"location":"kubernetes/shared-clusters/admin-access/#run-an-artisan-command","text":"kubectl exec -it $APP_POD -- php artisan migrate","title":"Run an artisan command"},{"location":"kubernetes/shared-clusters/admin-access/#run-an-sql-query","text":"echo 'SELECT * FROM users' | kubectl exec -i $DB_POD -- psql -U admin laravel","title":"Run an SQL query"},{"location":"kubernetes/shared-clusters/admin-access/#forward-postgresql-port","text":"kubectl port-forward pods/ $DB_POD 5432 :5432 Database logins Default database credentials can usually be found in helm-chart/values.yaml","title":"Forward PostgreSQL port"},{"location":"kubernetes/shared-clusters/create-cluster/","text":"Create a cluster \u00b6 Examples \u00b6 cluster-template : Jarvus\u2019 public template for lightweight, self-sufficient, multi-project Kubernetes clusters. jarvus-sandbox-cluster : If you have access, this repository demonstrates the setup documented in this section. Set up cluster repository \u00b6 Create a new repo (e.g. cluster-live ) and configure it with hologit, for example: git init cluster-live && cd cluster-live touch README.md && git add . && git commit -m \"wip: initial commit\" git holo init && git commit -m \"feat: configure holo workspace\" Add cluster-template as holosource in .holo/sources .holo/sources/cluster-template.toml [holosource] url = \"https://github.com/JarvusInnovations/cluster-template\" ref = \"refs/tags/v0.4.0\" Add holobranch for cluster-template using the k8s-blueprint holomapping in .holo/branches .holo/branches/k8s-manifests/_cluster-template.toml [holomapping] holosource = \"=>k8s-blueprint\" files = \"**\" before = \"*\" Add your own k8s manifests/settings to the repo e.g. Cluster Issuers for cert-manager certificates cert-manager.issuers.yaml apiVersion : cert-manager.io/v1alpha2 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : email : email@example.com server : https://acme-staging-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-staging solvers : - http01 : ingress : class : nginx --- apiVersion : cert-manager.io/v1alpha2 kind : ClusterIssuer metadata : name : letsencrypt-prod spec : acme : email : email@example.com server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-prod solvers : - http01 : ingress : class : nginx Project manifests into k8s/manifests branch git holo project --working k8s-manifests --commit-to = k8s/manifests Checkout the branch, and diff/apply the changes in the repo to the cluster. git checkout k8s/manifests kubectl diff -Rf ./ kubectl apply -Rf ./ Create GitHub token for deploy workflows \u00b6 The GitHub Actions deployment workflow requires a GitHub bot user to write to the cluster repository with. This is necessary so that a branch pushed by the GitHub Actions workflow can trigger subsequent workflows. The authentication tokens that are automatically provided to all GitHub Actions workflow runs are able to push to the host repository, but their pushes are prevented from triggering any workflows. Create or reuse GitHub bot user with write access to the cluster repository Grant the GitHub bot user the write access to the cluster repository, and ensure that its invitation is accepted Create a Personal Access Token under the GitHub bot user with the repo and workflow scopes Save the generated Personal Access Token to a secret called BOT_GITHUB_TOKEN under the cluster repository Create service account \u00b6 The GitHub Actions Workflows driving deployments will need a service account with read/write access to all namespaces. Add this manifest to the cluster\u2019s GitHub repository under e.g. deployers/cluster.yaml where it will become part of the automated deployment, but you will need to apply it to the cluster manually ahead of the first automated deployment: deployers/cluster.yaml apiVersion : v1 kind : ServiceAccount metadata : name : cluster-deployer namespace : kube-system --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : cluster-deployer rules : - apiGroups : [ \"*\" ] resources : [ \"*\" ] verbs : [ \"*\" ] --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : cluster-deployer namespace : kube-system roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-deployer subjects : - kind : ServiceAccount name : cluster-deployer namespace : kube-system Install mkkubeconfig command (if needed): sudo hab pkg install --binlink jarvus/mkkubeconfig Apply manifests to create service account: kubectl apply -f deployers/cluster.yaml Use mkkubeconfig to generate a base64-encoded KUBECONFIG for GitHub: mkkubeconfig kube-system cluster-deployer | base64 Save the base64 blob output above to a secret called KUBECONFIG_BASE64 under the cluster repository Create GitHub token for cluster image pulls \u00b6 If deployments will utilize Docker container images uploaded to GitHub\u2019s Packages registry, the deployment workflow can handle injecting these into any desired namespace. Create or reuse GitHub bot user with write access to the cluster repository Grant the GitHub bot user write access to the cluster repository, and ensure that its invitation is accepted Create a Personal Access Token under the GitHub bot user with the read:packages scope Generate a base64-encoded .docker/config.json file: echo -n 'Username: ' && read github_username echo -n 'Password: ' && read -s github_token echo && echo \"{ \\\"auths\\\": { \\\"docker.pkg.github.com\\\": { \\\"auth\\\": \\\" $( echo -n \" ${ github_username } : ${ github_token } \" | base64 ) \\\" }, \\\"ghcr.io\\\": { \\\"auth\\\": \\\" $( echo -n \" ${ github_username } : ${ github_token } \" | base64 ) \\\" } } }\" | base64 Save the generated base64 blob to a secret called DOCKER_CONFIG_BASE64 under the cluster repository","title":"Create a cluster"},{"location":"kubernetes/shared-clusters/create-cluster/#create-a-cluster","text":"","title":"Create a cluster"},{"location":"kubernetes/shared-clusters/create-cluster/#examples","text":"cluster-template : Jarvus\u2019 public template for lightweight, self-sufficient, multi-project Kubernetes clusters. jarvus-sandbox-cluster : If you have access, this repository demonstrates the setup documented in this section.","title":"Examples"},{"location":"kubernetes/shared-clusters/create-cluster/#set-up-cluster-repository","text":"Create a new repo (e.g. cluster-live ) and configure it with hologit, for example: git init cluster-live && cd cluster-live touch README.md && git add . && git commit -m \"wip: initial commit\" git holo init && git commit -m \"feat: configure holo workspace\" Add cluster-template as holosource in .holo/sources .holo/sources/cluster-template.toml [holosource] url = \"https://github.com/JarvusInnovations/cluster-template\" ref = \"refs/tags/v0.4.0\" Add holobranch for cluster-template using the k8s-blueprint holomapping in .holo/branches .holo/branches/k8s-manifests/_cluster-template.toml [holomapping] holosource = \"=>k8s-blueprint\" files = \"**\" before = \"*\" Add your own k8s manifests/settings to the repo e.g. Cluster Issuers for cert-manager certificates cert-manager.issuers.yaml apiVersion : cert-manager.io/v1alpha2 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : email : email@example.com server : https://acme-staging-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-staging solvers : - http01 : ingress : class : nginx --- apiVersion : cert-manager.io/v1alpha2 kind : ClusterIssuer metadata : name : letsencrypt-prod spec : acme : email : email@example.com server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-prod solvers : - http01 : ingress : class : nginx Project manifests into k8s/manifests branch git holo project --working k8s-manifests --commit-to = k8s/manifests Checkout the branch, and diff/apply the changes in the repo to the cluster. git checkout k8s/manifests kubectl diff -Rf ./ kubectl apply -Rf ./","title":"Set up cluster repository"},{"location":"kubernetes/shared-clusters/create-cluster/#create-github-token-for-deploy-workflows","text":"The GitHub Actions deployment workflow requires a GitHub bot user to write to the cluster repository with. This is necessary so that a branch pushed by the GitHub Actions workflow can trigger subsequent workflows. The authentication tokens that are automatically provided to all GitHub Actions workflow runs are able to push to the host repository, but their pushes are prevented from triggering any workflows. Create or reuse GitHub bot user with write access to the cluster repository Grant the GitHub bot user the write access to the cluster repository, and ensure that its invitation is accepted Create a Personal Access Token under the GitHub bot user with the repo and workflow scopes Save the generated Personal Access Token to a secret called BOT_GITHUB_TOKEN under the cluster repository","title":"Create GitHub token for deploy workflows"},{"location":"kubernetes/shared-clusters/create-cluster/#create-service-account","text":"The GitHub Actions Workflows driving deployments will need a service account with read/write access to all namespaces. Add this manifest to the cluster\u2019s GitHub repository under e.g. deployers/cluster.yaml where it will become part of the automated deployment, but you will need to apply it to the cluster manually ahead of the first automated deployment: deployers/cluster.yaml apiVersion : v1 kind : ServiceAccount metadata : name : cluster-deployer namespace : kube-system --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : cluster-deployer rules : - apiGroups : [ \"*\" ] resources : [ \"*\" ] verbs : [ \"*\" ] --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : cluster-deployer namespace : kube-system roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-deployer subjects : - kind : ServiceAccount name : cluster-deployer namespace : kube-system Install mkkubeconfig command (if needed): sudo hab pkg install --binlink jarvus/mkkubeconfig Apply manifests to create service account: kubectl apply -f deployers/cluster.yaml Use mkkubeconfig to generate a base64-encoded KUBECONFIG for GitHub: mkkubeconfig kube-system cluster-deployer | base64 Save the base64 blob output above to a secret called KUBECONFIG_BASE64 under the cluster repository","title":"Create service account"},{"location":"kubernetes/shared-clusters/create-cluster/#create-github-token-for-cluster-image-pulls","text":"If deployments will utilize Docker container images uploaded to GitHub\u2019s Packages registry, the deployment workflow can handle injecting these into any desired namespace. Create or reuse GitHub bot user with write access to the cluster repository Grant the GitHub bot user write access to the cluster repository, and ensure that its invitation is accepted Create a Personal Access Token under the GitHub bot user with the read:packages scope Generate a base64-encoded .docker/config.json file: echo -n 'Username: ' && read github_username echo -n 'Password: ' && read -s github_token echo && echo \"{ \\\"auths\\\": { \\\"docker.pkg.github.com\\\": { \\\"auth\\\": \\\" $( echo -n \" ${ github_username } : ${ github_token } \" | base64 ) \\\" }, \\\"ghcr.io\\\": { \\\"auth\\\": \\\" $( echo -n \" ${ github_username } : ${ github_token } \" | base64 ) \\\" } } }\" | base64 Save the generated base64 blob to a secret called DOCKER_CONFIG_BASE64 under the cluster repository","title":"Create GitHub token for cluster image pulls"},{"location":"kubernetes/shared-clusters/deploy-project/","text":"Deploy a project \u00b6 Prerequisites \u00b6 An operational shared cluster A working Dockerfile at the root of the project repository Setup \u00b6 Publish Docker container via GitHub \u00b6 Within the project\u2019s GitHub repository: Build a Helm 3 chart at in a helm-chart/ directory at the root of the project Define a holobranch named helm-chart that outputs the contents of the helm-chart/ directory Create a GitHub Actions workflow triggered by v* tags to build and publish a Docker container image to the project\u2019s GitHub Packages Create project deployment directory \u00b6 Within the shared cluster\u2019s GitHub repository, create a top-level directory naming this project deployment: mkdir client-project Set up release values \u00b6 Inside the project deployment directory created in the cluster\u2019s repository above, create a YAML file to hold deployment-specific overrides to the Helm chart\u2019s values.yaml , including at least the current image+tag: client-project/release.yaml image : docker.pkg.github.com/jarvusinnovations/client-project/client-project:v1.2.3\" This will be where the image version gets bumped to deploy new versions to the shared cluster. You can add any other instance-specific overrides like hostnames as well. Populate deployment directory with project\u2019s helm-chart projection \u00b6 Still within the shared cluster\u2019s GitHub repository, define a holosource and holomapping to populate the project\u2019s directory created above with content from the project\u2019s helm-chart projection: .holo/sources/client-project.toml [holosource] url = \"https://github.com/JarvusInnovations/client-project.git\" ref = \"refs/heads/deploy\" .holo/branches/k8s-manifests/client-project.toml [holomapping] holosource = \"=>helm-chart\" files = \"**\" During the projection of the cluster\u2019s k8s-manifests holobranch, the client-project directory will be populated from the remote helm-chart holobranch from the client-project holosource. The identically-named directory within the cluster\u2019s repository is then overlaid on top via an existing holomapping to copy all the cluster repository\u2019s overrides with after = \"*\" configured. Test composition \u00b6 To test the set up so far, project the k8s-manifests holobranch without lensing and list the full contents of the resulting project deployment directory: git ls-tree -r $( git holo project --working --no-lens k8s-manifests ) :client-project The output should include Chart.yaml , values.yaml , and templates/ from the project repository\u2019s helm-chart sources, plus the release.yaml file from the shared cluster repository\u2019s overrides directory: info: indexing working tree (this can take a while under Docker)... ... info: writing final output tree... info: projection ready 100644 blob 539bef2f8faea713a589ffabf97f094314faaf44 Chart.yaml 100644 blob dbf8c7a6ffc8edd87788ef885adf68cbf556f238 release.yaml 100644 blob dfcff8182969fe2db44037ed1c3a582b362bcdcc templates/_helpers.tpl 100644 blob e7e1a8bbbb63dc859dbf283557c413ae3e7c776f templates/cronjob.yaml 100644 blob 8fd6234ea1b1e2b04e469ef8ea31a860ec068901 templates/deployment.yaml 100644 blob b0c972120c53b742397d9032a8e14222b2aa14ef templates/ingress.yaml 100644 blob 0b2daeff5d1416eb981d6fa03dba5eabbebb6e73 templates/pvc.yaml 100644 blob 51ecaa3981401a8f20f56c458d92a9e46a3b3b8c templates/service.yaml 100644 blob 9a7fb93863e295ee8f059f3607840d7bf222b98d values.yaml Define hololens to build chart \u00b6 Finally, define a lens that uses helm template to transform the combined Helm chart content to deploy-ready Kubernetes manifests: .holo/lenses/client-project.toml [hololens] package = \"holo/lens-helm3/1.2\" [hololens.input] root = \"client-project\" files = \"**\" [hololens.output] merge = \"replace\" [hololens.helm] release_name = \"client-project\" namespace = \"client\" namespace_fill = true chart_path = \".\" value_files = [ \"values.yaml\" , \"release.yaml\" ] Test composition+lensing \u00b6 To test the full manifest generation process, first sync your local k8s/deploy branch to the current remote version: git update-ref refs/heads/k8s/deploy origin/k8s/deploy Then commit the projected k8s-manifests holobranch with lensing on top to preview how it will be extended: git holo project k8s-manifests-github --working --commit-to = k8s/deploy Add GitHub credentials to new namespace \u00b6 Authentication is always required to pull Docker images from GitHub Packages, even when the project repository is public. Wherever the project\u2019s Helm chart templates reference the project\u2019s Docker container image published on GitHub, imagePullSecrets should be configured to use a secret named regcred . The shared cluster\u2019s .github/workflows/deploy-manifests.yml workflow must then be edited to add a line with the new project namespace to the REGCRED_NAMESPACES env value. With that, the workflow will handle generating a regcred secret with access to GitHub in each namespace to include in every deployment. Authenticate access to private project sources \u00b6 If the project\u2019s repository is private, an additional step must be taken to grant fetch access to the GitHub Actions workflow running under the shared cluster\u2019s repository. Edit the shared cluster\u2019s .github/workflows/build-manifests.yml workflow and add an environment variable to the manifests projection step overriding the named source with an authenticated Git remote URL: # ... jobs : build-manifests : # ... steps : - name : 'Update holobranch: k8s/manifests' uses : JarvusInnovations/hologit@actions/projector/v1 env : # ... HOLO_SOURCE_CLIENT_PROJECT : https://jarvus-bot:${{ secrets.BOT_GITHUB_TOKEN }}@github.com/JarvusInnovations/client-project.git The bot user must be granted read access to the repository.","title":"Deploy a project"},{"location":"kubernetes/shared-clusters/deploy-project/#deploy-a-project","text":"","title":"Deploy a project"},{"location":"kubernetes/shared-clusters/deploy-project/#prerequisites","text":"An operational shared cluster A working Dockerfile at the root of the project repository","title":"Prerequisites"},{"location":"kubernetes/shared-clusters/deploy-project/#setup","text":"","title":"Setup"},{"location":"kubernetes/shared-clusters/deploy-project/#publish-docker-container-via-github","text":"Within the project\u2019s GitHub repository: Build a Helm 3 chart at in a helm-chart/ directory at the root of the project Define a holobranch named helm-chart that outputs the contents of the helm-chart/ directory Create a GitHub Actions workflow triggered by v* tags to build and publish a Docker container image to the project\u2019s GitHub Packages","title":"Publish Docker container via GitHub"},{"location":"kubernetes/shared-clusters/deploy-project/#create-project-deployment-directory","text":"Within the shared cluster\u2019s GitHub repository, create a top-level directory naming this project deployment: mkdir client-project","title":"Create project deployment directory"},{"location":"kubernetes/shared-clusters/deploy-project/#set-up-release-values","text":"Inside the project deployment directory created in the cluster\u2019s repository above, create a YAML file to hold deployment-specific overrides to the Helm chart\u2019s values.yaml , including at least the current image+tag: client-project/release.yaml image : docker.pkg.github.com/jarvusinnovations/client-project/client-project:v1.2.3\" This will be where the image version gets bumped to deploy new versions to the shared cluster. You can add any other instance-specific overrides like hostnames as well.","title":"Set up release values"},{"location":"kubernetes/shared-clusters/deploy-project/#populate-deployment-directory-with-projects-helm-chart-projection","text":"Still within the shared cluster\u2019s GitHub repository, define a holosource and holomapping to populate the project\u2019s directory created above with content from the project\u2019s helm-chart projection: .holo/sources/client-project.toml [holosource] url = \"https://github.com/JarvusInnovations/client-project.git\" ref = \"refs/heads/deploy\" .holo/branches/k8s-manifests/client-project.toml [holomapping] holosource = \"=>helm-chart\" files = \"**\" During the projection of the cluster\u2019s k8s-manifests holobranch, the client-project directory will be populated from the remote helm-chart holobranch from the client-project holosource. The identically-named directory within the cluster\u2019s repository is then overlaid on top via an existing holomapping to copy all the cluster repository\u2019s overrides with after = \"*\" configured.","title":"Populate deployment directory with project's helm-chart projection"},{"location":"kubernetes/shared-clusters/deploy-project/#test-composition","text":"To test the set up so far, project the k8s-manifests holobranch without lensing and list the full contents of the resulting project deployment directory: git ls-tree -r $( git holo project --working --no-lens k8s-manifests ) :client-project The output should include Chart.yaml , values.yaml , and templates/ from the project repository\u2019s helm-chart sources, plus the release.yaml file from the shared cluster repository\u2019s overrides directory: info: indexing working tree (this can take a while under Docker)... ... info: writing final output tree... info: projection ready 100644 blob 539bef2f8faea713a589ffabf97f094314faaf44 Chart.yaml 100644 blob dbf8c7a6ffc8edd87788ef885adf68cbf556f238 release.yaml 100644 blob dfcff8182969fe2db44037ed1c3a582b362bcdcc templates/_helpers.tpl 100644 blob e7e1a8bbbb63dc859dbf283557c413ae3e7c776f templates/cronjob.yaml 100644 blob 8fd6234ea1b1e2b04e469ef8ea31a860ec068901 templates/deployment.yaml 100644 blob b0c972120c53b742397d9032a8e14222b2aa14ef templates/ingress.yaml 100644 blob 0b2daeff5d1416eb981d6fa03dba5eabbebb6e73 templates/pvc.yaml 100644 blob 51ecaa3981401a8f20f56c458d92a9e46a3b3b8c templates/service.yaml 100644 blob 9a7fb93863e295ee8f059f3607840d7bf222b98d values.yaml","title":"Test composition"},{"location":"kubernetes/shared-clusters/deploy-project/#define-hololens-to-build-chart","text":"Finally, define a lens that uses helm template to transform the combined Helm chart content to deploy-ready Kubernetes manifests: .holo/lenses/client-project.toml [hololens] package = \"holo/lens-helm3/1.2\" [hololens.input] root = \"client-project\" files = \"**\" [hololens.output] merge = \"replace\" [hololens.helm] release_name = \"client-project\" namespace = \"client\" namespace_fill = true chart_path = \".\" value_files = [ \"values.yaml\" , \"release.yaml\" ]","title":"Define hololens to build chart"},{"location":"kubernetes/shared-clusters/deploy-project/#test-compositionlensing","text":"To test the full manifest generation process, first sync your local k8s/deploy branch to the current remote version: git update-ref refs/heads/k8s/deploy origin/k8s/deploy Then commit the projected k8s-manifests holobranch with lensing on top to preview how it will be extended: git holo project k8s-manifests-github --working --commit-to = k8s/deploy","title":"Test composition+lensing"},{"location":"kubernetes/shared-clusters/deploy-project/#add-github-credentials-to-new-namespace","text":"Authentication is always required to pull Docker images from GitHub Packages, even when the project repository is public. Wherever the project\u2019s Helm chart templates reference the project\u2019s Docker container image published on GitHub, imagePullSecrets should be configured to use a secret named regcred . The shared cluster\u2019s .github/workflows/deploy-manifests.yml workflow must then be edited to add a line with the new project namespace to the REGCRED_NAMESPACES env value. With that, the workflow will handle generating a regcred secret with access to GitHub in each namespace to include in every deployment.","title":"Add GitHub credentials to new namespace"},{"location":"kubernetes/shared-clusters/deploy-project/#authenticate-access-to-private-project-sources","text":"If the project\u2019s repository is private, an additional step must be taken to grant fetch access to the GitHub Actions workflow running under the shared cluster\u2019s repository. Edit the shared cluster\u2019s .github/workflows/build-manifests.yml workflow and add an environment variable to the manifests projection step overriding the named source with an authenticated Git remote URL: # ... jobs : build-manifests : # ... steps : - name : 'Update holobranch: k8s/manifests' uses : JarvusInnovations/hologit@actions/projector/v1 env : # ... HOLO_SOURCE_CLIENT_PROJECT : https://jarvus-bot:${{ secrets.BOT_GITHUB_TOKEN }}@github.com/JarvusInnovations/client-project.git The bot user must be granted read access to the repository.","title":"Authenticate access to private project sources"},{"location":"kubernetes/shared-clusters/deploy-prs/","text":"Deploy Pull Requests \u00b6 Enable a project GitHub repository to automatically deploy each Pull Request into a dedicated namespace hosted on an external shared cluster Prerequisites \u00b6 An operational shared cluster A working Dockerfile at the root of the project repository Setup \u00b6 Add a service account \u00b6 The GitHub Actions Workflows driving PR deployments will need a service account with read/write access to the dedicated namespace that all PR environments will be deployed to. Add this manifest to the cluster\u2019s GitHub repository under e.g. deployers/myproject.yaml where it will become part of the automated deployment: deployers/myproject.yaml apiVersion : v1 kind : Namespace metadata : name : myproject --- apiVersion : v1 kind : ServiceAccount metadata : name : deployer namespace : myproject --- kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : deployer namespace : myproject rules : - apiGroups : [ \"*\" ] resources : [ \"*\" ] verbs : [ \"*\" ] --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : deployer namespace : myproject subjects : - kind : ServiceAccount name : deployer namespace : myproject roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : deployer Generate KUBECONFIG \u00b6 After the service account manifest has been merged and deployed, you can generate a KUBECONFIG that client code can use to connect to the cluster under it: Install mkkubeconfig command (if needed): sudo hab pkg install --binlink jarvus/mkkubeconfig Use mkkubeconfig to generate a base64-encoded KUBECONFIG for GitHub: mkkubeconfig myproject deployer | base64 Copy the base64-encoded blob following the status log lines and paste it into a secret in the project GitHub repository named KUBECONFIG_BASE64 Install k8s-deploy.yml and k8s-destroy.yml into .github/workflows/ within the project GitHub repository and tailor the environment variables at the top","title":"Deploy Pull Requests"},{"location":"kubernetes/shared-clusters/deploy-prs/#deploy-pull-requests","text":"Enable a project GitHub repository to automatically deploy each Pull Request into a dedicated namespace hosted on an external shared cluster","title":"Deploy Pull Requests"},{"location":"kubernetes/shared-clusters/deploy-prs/#prerequisites","text":"An operational shared cluster A working Dockerfile at the root of the project repository","title":"Prerequisites"},{"location":"kubernetes/shared-clusters/deploy-prs/#setup","text":"","title":"Setup"},{"location":"kubernetes/shared-clusters/deploy-prs/#add-a-service-account","text":"The GitHub Actions Workflows driving PR deployments will need a service account with read/write access to the dedicated namespace that all PR environments will be deployed to. Add this manifest to the cluster\u2019s GitHub repository under e.g. deployers/myproject.yaml where it will become part of the automated deployment: deployers/myproject.yaml apiVersion : v1 kind : Namespace metadata : name : myproject --- apiVersion : v1 kind : ServiceAccount metadata : name : deployer namespace : myproject --- kind : Role apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : deployer namespace : myproject rules : - apiGroups : [ \"*\" ] resources : [ \"*\" ] verbs : [ \"*\" ] --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : deployer namespace : myproject subjects : - kind : ServiceAccount name : deployer namespace : myproject roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : deployer","title":"Add a service account"},{"location":"kubernetes/shared-clusters/deploy-prs/#generate-kubeconfig","text":"After the service account manifest has been merged and deployed, you can generate a KUBECONFIG that client code can use to connect to the cluster under it: Install mkkubeconfig command (if needed): sudo hab pkg install --binlink jarvus/mkkubeconfig Use mkkubeconfig to generate a base64-encoded KUBECONFIG for GitHub: mkkubeconfig myproject deployer | base64 Copy the base64-encoded blob following the status log lines and paste it into a secret in the project GitHub repository named KUBECONFIG_BASE64 Install k8s-deploy.yml and k8s-destroy.yml into .github/workflows/ within the project GitHub repository and tailor the environment variables at the top","title":"Generate KUBECONFIG"}]}